{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "Homework3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3d65kKYoMs5",
        "colab_type": "text"
      },
      "source": [
        "# Home 3: Build a CNN for image recognition.\n",
        "\n",
        "### Name: Zubair Shaikh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU4fZ2-koMs8",
        "colab_type": "text"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read, complete, and run the code.\n",
        "\n",
        "2. **Make substantial improvements** to maximize the accurcy.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    \n",
        "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo.\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583-2019F/blob/master/homework/HM3/HM3.html\n",
        "\n",
        "\n",
        "## Requirements:\n",
        "\n",
        "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
        "\n",
        "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
        "\n",
        "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
        "\n",
        "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
        "\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
        "\n",
        "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
        "\n",
        "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
        "\n",
        "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVkvmOnnoMs9",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGjqCsw9oMs-",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmR2Qlm6oMs_",
        "colab_type": "code",
        "outputId": "0d09e116-68a7-475a-cfd6-4f6c0e220ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CyveC_AoMtE",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a27YDlhMoMtG",
        "colab_type": "code",
        "outputId": "a23ddde6-b2bd-4773-f529-836f0dd1d977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "  return numpy.eye(num_class)[y.reshape(-1)]\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3e8JEEToMtL",
        "colab_type": "text"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpwktdHjoMtM",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets:\n",
        "* a training set containing 40K samples\n",
        "* a validation set containing 10K samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZPVbLXdoMtO",
        "colab_type": "code",
        "outputId": "91520eb4-a1cf-43dc-8a23-28168d2740a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "rand_indices = numpy.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0OAw-gLoMtS",
        "colab_type": "text"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters\n",
        "\n",
        "1. Build a convolutional neural network model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "3. Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGAvFfTFoMtU",
        "colab_type": "text"
      },
      "source": [
        "### Remark: \n",
        "\n",
        "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
        "* Add more layers.\n",
        "* Use regularizations, e.g., dropout.\n",
        "* Use batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv0HdYUpoMtW",
        "colab_type": "code",
        "outputId": "81243a02-7595-4343-c10b-283fdcc1a366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 15, 15, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 13, 13, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,253,674\n",
            "Trainable params: 1,252,266\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeKFO-Ptg9Wn",
        "colab_type": "text"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef-wu0cLfY9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=0,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=True    \n",
        ")\n",
        "\n",
        "train_generator = datagen.flow(x_tr, y_tr, batch_size=32)\n",
        "validation_generator = datagen.flow(x_val, y_val, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q9bAny8hwQs",
        "colab_type": "code",
        "outputId": "f0df3209-812b-415d-f315-3274922b27bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "learning_rate = 1E-5\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(train_generator, \n",
        "                              epochs=100,\n",
        "                              validation_data=validation_generator\n",
        "                              )\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 2.2718 - acc: 0.2018 - val_loss: 1.8877 - val_acc: 0.3186\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.9500 - acc: 0.2997 - val_loss: 1.7260 - val_acc: 0.3699\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.8188 - acc: 0.3498 - val_loss: 1.6175 - val_acc: 0.4089\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.7316 - acc: 0.3749 - val_loss: 1.5361 - val_acc: 0.4358\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.6628 - acc: 0.3991 - val_loss: 1.4839 - val_acc: 0.4611\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.6153 - acc: 0.4139 - val_loss: 1.4426 - val_acc: 0.4719\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.5803 - acc: 0.4303 - val_loss: 1.4092 - val_acc: 0.4883\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.5456 - acc: 0.4422 - val_loss: 1.3809 - val_acc: 0.4987\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.5134 - acc: 0.4543 - val_loss: 1.3491 - val_acc: 0.5084\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4891 - acc: 0.4636 - val_loss: 1.3310 - val_acc: 0.5194\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4645 - acc: 0.4703 - val_loss: 1.3080 - val_acc: 0.5230\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4454 - acc: 0.4796 - val_loss: 1.2879 - val_acc: 0.5365\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4202 - acc: 0.4895 - val_loss: 1.2581 - val_acc: 0.5488\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.4035 - acc: 0.4937 - val_loss: 1.2454 - val_acc: 0.5581\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3843 - acc: 0.5020 - val_loss: 1.2424 - val_acc: 0.5513\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3733 - acc: 0.5062 - val_loss: 1.2237 - val_acc: 0.5588\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3573 - acc: 0.5147 - val_loss: 1.1989 - val_acc: 0.5673\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3447 - acc: 0.5190 - val_loss: 1.1945 - val_acc: 0.5762\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3251 - acc: 0.5237 - val_loss: 1.1812 - val_acc: 0.5823\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3167 - acc: 0.5290 - val_loss: 1.1741 - val_acc: 0.5810\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3056 - acc: 0.5312 - val_loss: 1.1497 - val_acc: 0.5893\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2953 - acc: 0.5359 - val_loss: 1.1407 - val_acc: 0.5929\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2790 - acc: 0.5406 - val_loss: 1.1374 - val_acc: 0.5902\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2722 - acc: 0.5467 - val_loss: 1.1289 - val_acc: 0.5973\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2563 - acc: 0.5501 - val_loss: 1.1059 - val_acc: 0.6059\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2492 - acc: 0.5534 - val_loss: 1.0966 - val_acc: 0.6113\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2365 - acc: 0.5592 - val_loss: 1.0934 - val_acc: 0.6034\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.2336 - acc: 0.5604 - val_loss: 1.0746 - val_acc: 0.6234\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2160 - acc: 0.5644 - val_loss: 1.0707 - val_acc: 0.6179\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2104 - acc: 0.5708 - val_loss: 1.0620 - val_acc: 0.6201\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 1.1950 - acc: 0.5753 - val_loss: 1.0602 - val_acc: 0.6184\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1913 - acc: 0.5757 - val_loss: 1.0467 - val_acc: 0.6316\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1772 - acc: 0.5792 - val_loss: 1.0427 - val_acc: 0.6276\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1714 - acc: 0.5843 - val_loss: 1.0302 - val_acc: 0.6373\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1689 - acc: 0.5853 - val_loss: 1.0235 - val_acc: 0.6351\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1621 - acc: 0.5870 - val_loss: 1.0227 - val_acc: 0.6400\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1522 - acc: 0.5905 - val_loss: 1.0046 - val_acc: 0.6466\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1471 - acc: 0.5931 - val_loss: 0.9998 - val_acc: 0.6479\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1439 - acc: 0.5945 - val_loss: 1.0016 - val_acc: 0.6464\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1271 - acc: 0.5994 - val_loss: 0.9862 - val_acc: 0.6485\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1205 - acc: 0.6054 - val_loss: 0.9913 - val_acc: 0.6486\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1154 - acc: 0.6035 - val_loss: 0.9788 - val_acc: 0.6518\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1139 - acc: 0.6069 - val_loss: 0.9780 - val_acc: 0.6509\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1016 - acc: 0.6109 - val_loss: 0.9620 - val_acc: 0.6615\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1007 - acc: 0.6107 - val_loss: 0.9673 - val_acc: 0.6553\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0870 - acc: 0.6155 - val_loss: 0.9546 - val_acc: 0.6589\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0859 - acc: 0.6154 - val_loss: 0.9529 - val_acc: 0.6639\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0832 - acc: 0.6184 - val_loss: 0.9444 - val_acc: 0.6665\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0782 - acc: 0.6199 - val_loss: 0.9445 - val_acc: 0.6674\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0694 - acc: 0.6237 - val_loss: 0.9353 - val_acc: 0.6662\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0601 - acc: 0.6271 - val_loss: 0.9260 - val_acc: 0.6763\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.0528 - acc: 0.6291 - val_loss: 0.9241 - val_acc: 0.6757\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.0465 - acc: 0.6299 - val_loss: 0.9191 - val_acc: 0.6695\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0438 - acc: 0.6306 - val_loss: 0.9197 - val_acc: 0.6744\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0346 - acc: 0.6334 - val_loss: 0.9071 - val_acc: 0.6805\n",
            "Epoch 56/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0391 - acc: 0.6353 - val_loss: 0.9047 - val_acc: 0.6807\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0318 - acc: 0.6338 - val_loss: 0.8986 - val_acc: 0.6819\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0273 - acc: 0.6361 - val_loss: 0.8982 - val_acc: 0.6839\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0242 - acc: 0.6413 - val_loss: 0.8986 - val_acc: 0.6847\n",
            "Epoch 60/100\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 1.0227 - acc: 0.6392 - val_loss: 0.8945 - val_acc: 0.6853\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0150 - acc: 0.6450 - val_loss: 0.8850 - val_acc: 0.6890\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0107 - acc: 0.6440 - val_loss: 0.8778 - val_acc: 0.6917\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0088 - acc: 0.6439 - val_loss: 0.8767 - val_acc: 0.6913\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0015 - acc: 0.6464 - val_loss: 0.8712 - val_acc: 0.6932\n",
            "Epoch 65/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0003 - acc: 0.6458 - val_loss: 0.8644 - val_acc: 0.6956\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0024 - acc: 0.6466 - val_loss: 0.8734 - val_acc: 0.6955\n",
            "Epoch 67/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9882 - acc: 0.6535 - val_loss: 0.8596 - val_acc: 0.6989\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9799 - acc: 0.6526 - val_loss: 0.8581 - val_acc: 0.6976\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9812 - acc: 0.6550 - val_loss: 0.8590 - val_acc: 0.6973\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9771 - acc: 0.6540 - val_loss: 0.8498 - val_acc: 0.7027\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 0.9705 - acc: 0.6583 - val_loss: 0.8470 - val_acc: 0.7027\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9713 - acc: 0.6575 - val_loss: 0.8513 - val_acc: 0.7024\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9698 - acc: 0.6558 - val_loss: 0.8373 - val_acc: 0.7054\n",
            "Epoch 74/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9685 - acc: 0.6589 - val_loss: 0.8469 - val_acc: 0.7012\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9619 - acc: 0.6606 - val_loss: 0.8388 - val_acc: 0.7054\n",
            "Epoch 76/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9581 - acc: 0.6643 - val_loss: 0.8341 - val_acc: 0.7095\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9549 - acc: 0.6605 - val_loss: 0.8260 - val_acc: 0.7088\n",
            "Epoch 78/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9533 - acc: 0.6666 - val_loss: 0.8283 - val_acc: 0.7107\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9503 - acc: 0.6664 - val_loss: 0.8265 - val_acc: 0.7103\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9415 - acc: 0.6675 - val_loss: 0.8191 - val_acc: 0.7088\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.9395 - acc: 0.6704 - val_loss: 0.8201 - val_acc: 0.7120\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.9353 - acc: 0.6699 - val_loss: 0.8164 - val_acc: 0.7120\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 0.9414 - acc: 0.6736 - val_loss: 0.8171 - val_acc: 0.7117\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 0.9343 - acc: 0.6711 - val_loss: 0.8121 - val_acc: 0.7158\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.9283 - acc: 0.6719 - val_loss: 0.8101 - val_acc: 0.7146\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.9237 - acc: 0.6760 - val_loss: 0.8053 - val_acc: 0.7162\n",
            "Epoch 87/100\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.9218 - acc: 0.6793 - val_loss: 0.8036 - val_acc: 0.7180\n",
            "Epoch 88/100\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 0.9162 - acc: 0.6792 - val_loss: 0.7954 - val_acc: 0.7251\n",
            "Epoch 89/100\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.9169 - acc: 0.6768 - val_loss: 0.7987 - val_acc: 0.7224\n",
            "Epoch 90/100\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 0.9105 - acc: 0.6803 - val_loss: 0.7936 - val_acc: 0.7243\n",
            "Epoch 91/100\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 0.9156 - acc: 0.6792 - val_loss: 0.7924 - val_acc: 0.7249\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 0.9091 - acc: 0.6821 - val_loss: 0.7885 - val_acc: 0.7280\n",
            "Epoch 93/100\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.9105 - acc: 0.6805 - val_loss: 0.7959 - val_acc: 0.7232\n",
            "Epoch 94/100\n",
            " 888/1250 [====================>.........] - ETA: 10s - loss: 0.9088 - acc: 0.6832Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsuLiO_MmOys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "be48b00c-45ca-4c47-d91c-a428d874683c"
      },
      "source": [
        "print(\"\\n\\n*********************************************************************\")\n",
        "print(\"learning rate: {0}, loss: {1}, acc: {2}, val_loss: {3}, val_acc: {4}\".format(learning_rate, \n",
        "                                                                                    history.history['loss'][-1],\n",
        "                                                                                    history.history['acc'][-1],\n",
        "                                                                                    history.history['val_loss'][-1],\n",
        "                                                                                    history.history['val_acc'][-1]\n",
        "                                                                                    ))\n",
        "print(\"*********************************************************************\\n\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "*********************************************************************\n",
            "learning rate: 1e-05, loss: 0.888323738527298, acc: 0.691625, val_loss: 0.7784737033843994, val_acc: 0.7244\n",
            "*********************************************************************\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgaQAT1KoMti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "208714b0-ffe5-425b-fdcc-b4639d3e9850"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfM4CgTHG+Nz/B8nSTPPMKiVDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SomXA-mLoMtr",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train (again) and evaluate the model\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWstzZusoMtt",
        "colab_type": "text"
      },
      "source": [
        "### 3.1. Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOEX3Q9OoMtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <Compile your model again (using the same hyper-parameters)>\n",
        "# ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlyBeDGExgQo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa1a7f20-b97a-4926-8ad8-4855d4d87c9b"
      },
      "source": [
        "train_generator = datagen.flow(x_train, y_train_vec, batch_size=32)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(train_generator, \n",
        "                              epochs=100)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: 0.8907 - acc: 0.6857\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8911 - acc: 0.6889\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8864 - acc: 0.6911\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8777 - acc: 0.6906\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8786 - acc: 0.6917\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8765 - acc: 0.6941\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: 0.8697 - acc: 0.6941\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8713 - acc: 0.6942\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8669 - acc: 0.6967\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8647 - acc: 0.6964\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8600 - acc: 0.6993\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8659 - acc: 0.6953\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8508 - acc: 0.7016\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8559 - acc: 0.7022\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8509 - acc: 0.7044\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8484 - acc: 0.7022\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8532 - acc: 0.7018\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8485 - acc: 0.7043\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8403 - acc: 0.7068\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.8414 - acc: 0.7033\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8369 - acc: 0.7071\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8343 - acc: 0.7079\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8344 - acc: 0.7102\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8329 - acc: 0.7082\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8316 - acc: 0.7089\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8293 - acc: 0.7105\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8272 - acc: 0.7104\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8229 - acc: 0.7122\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8224 - acc: 0.7135\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8192 - acc: 0.7144\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8182 - acc: 0.7169\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8152 - acc: 0.7167\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8088 - acc: 0.7173\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8127 - acc: 0.7157\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8028 - acc: 0.7165\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8048 - acc: 0.7199\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8073 - acc: 0.7196\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8047 - acc: 0.7196\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8058 - acc: 0.7207\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8012 - acc: 0.7205\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7967 - acc: 0.7232\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8005 - acc: 0.7191\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7894 - acc: 0.7250\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7915 - acc: 0.7228\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7854 - acc: 0.7259\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7907 - acc: 0.7240\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7865 - acc: 0.7250\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7860 - acc: 0.7250\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7854 - acc: 0.7258\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7789 - acc: 0.7258\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7816 - acc: 0.7293\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7757 - acc: 0.7286\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7762 - acc: 0.7280\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7713 - acc: 0.7313\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7699 - acc: 0.7292\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7669 - acc: 0.7329\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7656 - acc: 0.7342\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7666 - acc: 0.7334\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7644 - acc: 0.7333\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7643 - acc: 0.7342\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7580 - acc: 0.7374\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7604 - acc: 0.7359\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7581 - acc: 0.7378\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7598 - acc: 0.7370\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7573 - acc: 0.7384\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7542 - acc: 0.7393\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7537 - acc: 0.7372\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7530 - acc: 0.7382\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7522 - acc: 0.7379\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7520 - acc: 0.7384\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7441 - acc: 0.7401\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7467 - acc: 0.7398\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7449 - acc: 0.7398\n",
            "Epoch 74/100\n",
            "1222/1563 [======================>.......] - ETA: 10s - loss: 0.7421 - acc: 0.7429Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTEs1eyMmRiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "d2c89836-2450-478d-bf9e-2cceb58a1157"
      },
      "source": [
        "print(\"\\n\\n*********************************************************************\")\n",
        "print(\"loss: {0}, acc: {1}\".format(history.history['loss'][-1], history.history['acc'][-1]))\n",
        "print(\"*********************************************************************\\n\\n\")   "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "*********************************************************************\n",
            "loss: 0.7088267396354675, acc: 0.75352\n",
            "*********************************************************************\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5cFxnbtoMt4",
        "colab_type": "text"
      },
      "source": [
        "### 3.2. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEcTMRHvoMt5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6b54b3f9-e77a-4c00-c124-2038edcae920"
      },
      "source": [
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 224us/step\n",
            "loss = 0.61788270611763\n",
            "accuracy = 0.7825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Q1ldcKoMt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}