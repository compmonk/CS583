{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "Homework3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3d65kKYoMs5",
        "colab_type": "text"
      },
      "source": [
        "# Home 3: Build a CNN for image recognition.\n",
        "\n",
        "### Name: Zubair Shaikh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU4fZ2-koMs8",
        "colab_type": "text"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read, complete, and run the code.\n",
        "\n",
        "2. **Make substantial improvements** to maximize the accurcy.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    \n",
        "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo.\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583-2019F/blob/master/homework/HM3/HM3.html\n",
        "\n",
        "\n",
        "## Requirements:\n",
        "\n",
        "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
        "\n",
        "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
        "\n",
        "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
        "\n",
        "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
        "\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
        "\n",
        "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
        "\n",
        "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
        "\n",
        "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVkvmOnnoMs9",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGjqCsw9oMs-",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmR2Qlm6oMs_",
        "colab_type": "code",
        "outputId": "c69cc2b9-6427-47f6-d037-15b8a28be63c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CyveC_AoMtE",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a27YDlhMoMtG",
        "colab_type": "code",
        "outputId": "87a48c82-17c4-4f40-c836-9eef9145bacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "  return numpy.eye(num_class)[y.reshape(-1)]\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3e8JEEToMtL",
        "colab_type": "text"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpwktdHjoMtM",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets:\n",
        "* a training set containing 40K samples\n",
        "* a validation set containing 10K samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZPVbLXdoMtO",
        "colab_type": "code",
        "outputId": "be61234d-da27-4544-f053-1508979e3fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "rand_indices = numpy.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0OAw-gLoMtS",
        "colab_type": "text"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters\n",
        "\n",
        "1. Build a convolutional neural network model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "3. Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGAvFfTFoMtU",
        "colab_type": "text"
      },
      "source": [
        "### Remark: \n",
        "\n",
        "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
        "* Add more layers.\n",
        "* Use regularizations, e.g., dropout.\n",
        "* Use batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv0HdYUpoMtW",
        "colab_type": "code",
        "outputId": "ace924b0-4cab-40ed-db07-3e4db5209226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 15, 15, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 13, 13, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,253,674\n",
            "Trainable params: 1,252,266\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeKFO-Ptg9Wn",
        "colab_type": "text"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef-wu0cLfY9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=0,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=True    \n",
        ")\n",
        "\n",
        "train_generator = datagen.flow(x_tr, y_tr, batch_size=32)\n",
        "validation_generator = datagen.flow(x_val, y_val, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q9bAny8hwQs",
        "colab_type": "code",
        "outputId": "4c8c3dfe-68dc-4659-d34c-e3f8c4a08fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "learning_rate = 1E-5\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(train_generator, \n",
        "                              epochs=100,\n",
        "                              validation_data=validation_generator\n",
        "                              )\n",
        "\n",
        "print(\"\\n\\n*********************************************************************\")\n",
        "print(\"learning rate: {0}, loss: {1}, acc: {2}, val_loss: {3}, val_acc: {4}\".format(learning_rate, \n",
        "                                                                                    history.history['loss'][-1],\n",
        "                                                                                    history.history['acc'][-1],\n",
        "                                                                                    history.history['val_loss'][-1],\n",
        "                                                                                    history.history['val_acc'][-1]\n",
        "                                                                                    ))\n",
        "print(\"*********************************************************************\\n\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 2.2185 - acc: 0.2078 - val_loss: 1.8526 - val_acc: 0.3338\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 1.9131 - acc: 0.3118 - val_loss: 1.6887 - val_acc: 0.3920\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.7945 - acc: 0.3518 - val_loss: 1.6083 - val_acc: 0.4150\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 1.7098 - acc: 0.3792 - val_loss: 1.5400 - val_acc: 0.4391\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 1.6563 - acc: 0.4004 - val_loss: 1.4988 - val_acc: 0.4567\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.6056 - acc: 0.4159 - val_loss: 1.4630 - val_acc: 0.4742\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 1.5764 - acc: 0.4292 - val_loss: 1.4320 - val_acc: 0.4809\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.5368 - acc: 0.4420 - val_loss: 1.4062 - val_acc: 0.4915\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.5130 - acc: 0.4515 - val_loss: 1.3841 - val_acc: 0.4984\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.4847 - acc: 0.4644 - val_loss: 1.3621 - val_acc: 0.5123\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.4618 - acc: 0.4739 - val_loss: 1.3450 - val_acc: 0.5152\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 1.4459 - acc: 0.4767 - val_loss: 1.3317 - val_acc: 0.5251\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.4247 - acc: 0.4852 - val_loss: 1.3126 - val_acc: 0.5311\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4118 - acc: 0.4930 - val_loss: 1.2952 - val_acc: 0.5345\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 1.3905 - acc: 0.4970 - val_loss: 1.2820 - val_acc: 0.5403\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3786 - acc: 0.5032 - val_loss: 1.2653 - val_acc: 0.5501\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3577 - acc: 0.5144 - val_loss: 1.2481 - val_acc: 0.5547\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3451 - acc: 0.5194 - val_loss: 1.2460 - val_acc: 0.5573\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3317 - acc: 0.5214 - val_loss: 1.2208 - val_acc: 0.5678\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3254 - acc: 0.5242 - val_loss: 1.2143 - val_acc: 0.5689\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3044 - acc: 0.5324 - val_loss: 1.2111 - val_acc: 0.5662\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2983 - acc: 0.5366 - val_loss: 1.1976 - val_acc: 0.5757\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2892 - acc: 0.5378 - val_loss: 1.1942 - val_acc: 0.5769\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2751 - acc: 0.5462 - val_loss: 1.1789 - val_acc: 0.5800\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2593 - acc: 0.5505 - val_loss: 1.1665 - val_acc: 0.5889\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2546 - acc: 0.5546 - val_loss: 1.1502 - val_acc: 0.5911\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2375 - acc: 0.5590 - val_loss: 1.1484 - val_acc: 0.5947\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2367 - acc: 0.5612 - val_loss: 1.1351 - val_acc: 0.6022\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2294 - acc: 0.5617 - val_loss: 1.1320 - val_acc: 0.5973\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2245 - acc: 0.5628 - val_loss: 1.1232 - val_acc: 0.6020\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2078 - acc: 0.5698 - val_loss: 1.1115 - val_acc: 0.6062\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1971 - acc: 0.5775 - val_loss: 1.1097 - val_acc: 0.6050\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1925 - acc: 0.5748 - val_loss: 1.1042 - val_acc: 0.6111\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1881 - acc: 0.5771 - val_loss: 1.0933 - val_acc: 0.6128\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1775 - acc: 0.5831 - val_loss: 1.0850 - val_acc: 0.6176\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1719 - acc: 0.5862 - val_loss: 1.0725 - val_acc: 0.6245\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.1594 - acc: 0.5887 - val_loss: 1.0741 - val_acc: 0.6217\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 48s 38ms/step - loss: 1.1614 - acc: 0.5909 - val_loss: 1.0568 - val_acc: 0.6298\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1529 - acc: 0.5921 - val_loss: 1.0582 - val_acc: 0.6248\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1426 - acc: 0.5956 - val_loss: 1.0541 - val_acc: 0.6269\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1376 - acc: 0.5984 - val_loss: 1.0483 - val_acc: 0.6319\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.1314 - acc: 0.6008 - val_loss: 1.0290 - val_acc: 0.6410\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1210 - acc: 0.6019 - val_loss: 1.0205 - val_acc: 0.6474\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1222 - acc: 0.6067 - val_loss: 1.0175 - val_acc: 0.6397\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 1.1140 - acc: 0.6016 - val_loss: 1.0287 - val_acc: 0.6369\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1051 - acc: 0.6083 - val_loss: 1.0162 - val_acc: 0.6411\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1010 - acc: 0.6101 - val_loss: 1.0159 - val_acc: 0.6451\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0969 - acc: 0.6143 - val_loss: 1.0112 - val_acc: 0.6477\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0949 - acc: 0.6111 - val_loss: 1.0116 - val_acc: 0.6484\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 1.0816 - acc: 0.6191 - val_loss: 0.9991 - val_acc: 0.6458\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0806 - acc: 0.6187 - val_loss: 0.9950 - val_acc: 0.6523\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0722 - acc: 0.6233 - val_loss: 0.9783 - val_acc: 0.6615\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0658 - acc: 0.6266 - val_loss: 0.9865 - val_acc: 0.6572\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0672 - acc: 0.6203 - val_loss: 0.9731 - val_acc: 0.6604\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 1.0645 - acc: 0.6229 - val_loss: 0.9768 - val_acc: 0.6515\n",
            "Epoch 56/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0539 - acc: 0.6279 - val_loss: 0.9653 - val_acc: 0.6634\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0549 - acc: 0.6287 - val_loss: 0.9598 - val_acc: 0.6615\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0495 - acc: 0.6291 - val_loss: 0.9592 - val_acc: 0.6633\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0418 - acc: 0.6304 - val_loss: 0.9593 - val_acc: 0.6660\n",
            "Epoch 60/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 1.0412 - acc: 0.6332 - val_loss: 0.9563 - val_acc: 0.6682\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 1.0311 - acc: 0.6381 - val_loss: 0.9473 - val_acc: 0.6713\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0260 - acc: 0.6409 - val_loss: 0.9412 - val_acc: 0.6725\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0202 - acc: 0.6409 - val_loss: 0.9305 - val_acc: 0.6733\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0217 - acc: 0.6399 - val_loss: 0.9247 - val_acc: 0.6830\n",
            "Epoch 65/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0210 - acc: 0.6393 - val_loss: 0.9229 - val_acc: 0.6747\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 1.0132 - acc: 0.6441 - val_loss: 0.9150 - val_acc: 0.6791\n",
            "Epoch 67/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0051 - acc: 0.6446 - val_loss: 0.9226 - val_acc: 0.6811\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 1.0074 - acc: 0.6432 - val_loss: 0.9153 - val_acc: 0.6806\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9989 - acc: 0.6472 - val_loss: 0.9167 - val_acc: 0.6809\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.0000 - acc: 0.6487 - val_loss: 0.9160 - val_acc: 0.6827\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9926 - acc: 0.6524 - val_loss: 0.9182 - val_acc: 0.6804\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9831 - acc: 0.6538 - val_loss: 0.9054 - val_acc: 0.6879\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9877 - acc: 0.6510 - val_loss: 0.9068 - val_acc: 0.6886\n",
            "Epoch 74/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9804 - acc: 0.6566 - val_loss: 0.9001 - val_acc: 0.6895\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9787 - acc: 0.6549 - val_loss: 0.8904 - val_acc: 0.6933\n",
            "Epoch 76/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 0.9818 - acc: 0.6551 - val_loss: 0.8955 - val_acc: 0.6917\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9731 - acc: 0.6564 - val_loss: 0.8856 - val_acc: 0.6937\n",
            "Epoch 78/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9710 - acc: 0.6595 - val_loss: 0.8852 - val_acc: 0.6900\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 0.9664 - acc: 0.6611 - val_loss: 0.8881 - val_acc: 0.6839\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9609 - acc: 0.6638 - val_loss: 0.8850 - val_acc: 0.6922\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9567 - acc: 0.6625 - val_loss: 0.8766 - val_acc: 0.6940\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9624 - acc: 0.6626 - val_loss: 0.8872 - val_acc: 0.6953\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 0.9559 - acc: 0.6634 - val_loss: 0.8768 - val_acc: 0.6999\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9505 - acc: 0.6657 - val_loss: 0.8757 - val_acc: 0.6950\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 0.9543 - acc: 0.6645 - val_loss: 0.8735 - val_acc: 0.6919\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 0.9462 - acc: 0.6682 - val_loss: 0.8620 - val_acc: 0.7025\n",
            "Epoch 87/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 0.9403 - acc: 0.6696 - val_loss: 0.8595 - val_acc: 0.7031\n",
            "Epoch 88/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9393 - acc: 0.6714 - val_loss: 0.8702 - val_acc: 0.7046\n",
            "Epoch 89/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9315 - acc: 0.6734 - val_loss: 0.8622 - val_acc: 0.7033\n",
            "Epoch 90/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9276 - acc: 0.6725 - val_loss: 0.8581 - val_acc: 0.7041\n",
            "Epoch 91/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 0.9389 - acc: 0.6715 - val_loss: 0.8597 - val_acc: 0.7046\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 0.9232 - acc: 0.6770 - val_loss: 0.8524 - val_acc: 0.7051\n",
            "Epoch 93/100\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9257 - acc: 0.6747 - val_loss: 0.8475 - val_acc: 0.7060\n",
            "Epoch 94/100\n",
            " 108/1250 [=>............................] - ETA: 36s - loss: 0.9117 - acc: 0.6774Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgaQAT1KoMti",
        "colab_type": "code",
        "outputId": "69bb94db-ef3d-4d5e-d317-560c817a8f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3xUZdbA8d9JKFKkCuICkoggmyA1\nIi4qgvIKqCCgLyAWbCxWbKsovGJfXQs2VsXCqrCiwoq4C1gQBVZdmoASLDQxLEoA6TXkvH88MzBJ\nZpJJuZnJ3PP9fOYzuXfu3HmG0Xvu084jqooxxhj/Sop1AYwxxsSWBQJjjPE5CwTGGONzFgiMMcbn\nLBAYY4zPVYp1AYrrmGOO0ZSUlFgXwxhjKpTFixdvVtUG4V6rcIEgJSWFRYsWxboYxhhToYjIT5Fe\ns6YhY4zxOQsExhjjcxYIjDHG5ypcH0E4Bw8eJCsri3379sW6KKYQRx11FE2aNKFy5cqxLooxJkRC\nBIKsrCyOPvpoUlJSEJFYF8eEoaps2bKFrKwsUlNTY10cY0yIhGga2rdvH/Xr17cgEMdEhPr161ut\nzZg4lBCBALAgUAHYb2RMfEqYQGCMMQnr0CG44w5Yt86T01sgKANbtmyhXbt2tGvXjkaNGtG4cePD\n2wcOHIjqHFdeeSXff/99oceMGzeOSZMmlUWRjTEVyZgx8OST8PHHnpw+ITqLi2vSJBg1Ctavh+OP\nh4cfhiFDSn6++vXrs3TpUgDuu+8+atasyR133JHnGFVFVUlKCh97J0yYUOTn3HDDDSUvpDEmtr76\nCt5/H2rXhvr1oUULOOusgscdOgTJyUe2p0xxF6lrrnEPD/iuRjBpEgwbBj/9BKruedgwt7+srVq1\nirS0NIYMGUJ6ejobN25k2LBhZGRkkJ6ezgMPPHD42NNPP52lS5eSk5NDnTp1GDlyJG3btuW0005j\n06ZNAIwePZqnn3768PEjR46kU6dOnHTSSXzxxRcA7N69mwEDBpCWlsZFF11ERkbG4SAVasyYMZxy\nyim0bt2a4cOHE1yp7ocffqB79+60bduWDh06sC5QFX3kkUc4+eSTadu2LaNGjSr7fyxjEtWvv8KV\nV8Jpp8Ff/gJ33+0uOt26uTv90FUiX3wRatSAfv1g3jz45hsYOhQ6d4bnnweP+tk8rRGISE/gGSAZ\neEVVH833+ligW2CzOtBQVet4WaZRo2DPnrz79uxx+0tTK4jku+++44033iAjIwOARx99lHr16pGT\nk0O3bt246KKLSEtLy/Oe7du307VrVx599FFuu+02XnvtNUaOHFng3KrKggULmD59Og888ACzZs3i\nueeeo1GjRkydOpVly5bRoUOHsOUaMWIE999/P6rKJZdcwqxZs+jVqxeDBw/mvvvu44ILLmDfvn3k\n5ubywQcfMHPmTBYsWEC1atXYunVr2f9DGRMPDhyAFSvcBbh2bUhPh9RUdwHesgX++1/YvBm2bTvy\n+O039/zrr/Dzz66pYe9ed9dfvz58/727yIwc6S40ycnuXPfeCw88ALm57nnMGHjwQejUyQWBadOg\nalWoVw+mTnV/e8SzQCAiycA4oAeQBSwUkemqmhk8RlVvDTn+JqC9V+UJWr++ePtLq3nz5oeDAMBb\nb73Fq6++Sk5ODv/973/JzMwsEAiqVatGr169AOjYsSPz5s0Le+7+/fsfPiZ45z5//nzuuusuANq2\nbUt6enrY986ePZvHH3+cffv2sXnzZjp27Ejnzp3ZvHkzF1xwAeAmgAF88sknXHXVVVSrVg2AevXq\nleSfwpj4sm8fPPSQu1Bv3gybNsGPP8LBg3mPq1rVXazz7w9KSoI6daBBA2jaFHr2dHf1W7e6C37P\nnnD//XDSSUfe06QJvPKKCwoPPQQzZsCSJXDVVfDSSy4gvfkmvPuuaxb63e+8+3fA2xpBJ2CVqq4B\nEJHJQF8gM8Lxg4ExHpYHcH0CP4XJwXf88d58Xo0aNQ7//eOPP/LMM8+wYMEC6tSpw6WXXhp2XH2V\nKlUO/52cnExOTk7Yc1cN3CEUdkw4e/bs4cYbb2TJkiU0btyY0aNH2/h+k1iWLHHt8cOHw3HHhT/m\njjtg3Dho1QqOOQZatoQ+faBdO2jTBnbsgMxMWLkSKlVy5znuOGjY0F3469Z1tYajjy5Zk01Skrvo\nJye759GjXc1AxH3eH//oHuXAy0DQGPg5ZDsLODXcgSLSDEgFPo3w+jBgGMDxpbxiP/ywa54LbR6q\nXt3t99qOHTs4+uijqVWrFhs3buTDDz+kZ8+eZfoZXbp04Z133uGMM87gm2++ITOzYNzdu3cvSUlJ\nHHPMMezcuZOpU6cyZMgQ6tatS4MGDfjggw/yNA316NGDxx57jEGDBh1uGrJagYlL2dmu+eWVV1zb\n+zPPwJ//7C6ooQM1pkxxQeD22+GJJyKf79Swl6yyk5QEL7wA99zj3d1oNMWI2SfnNQiYoqqHwr2o\nquNVNUNVMxo0CLuuQtSGDIHx46FZMxd4mzVz2170D+TXoUMH0tLSaNWqFZdffjldunQp88+46aab\n2LBhA2lpadx///2kpaVRu3btPMfUr1+fK664grS0NHr16sWpIf+xT5o0iSeffJI2bdpw+umnk52d\nzfnnn0/Pnj3JyMigXbt2jB07tszLbQzgmlJ2747uWFVYvRreftu1r198sRuJM2EC3HILLFgAHTvC\n9de7jtopU1yTy5o1cPXV7iL/yCPefp9oiMQ0CABHhjWW9QM4DfgwZPtu4O4Ix34N/CGa83bs2FHz\ny8zMLLDPrw4ePKh79+5VVdUffvhBU1JS9ODBgzEu1RH2W/nctm2qjzyi2qiRapMmqhdeqPrgg6o3\n3qianq4KqrVrq95zj+ovv4Q/x7x57n0NG7rjQTUpSbVFC9XBg1VD/xvLzVWdOFG1aVN3XIMGqs2b\nu89Yu7ZcvnK8ABZphOuql01DC4EWIpIKbMDd9V+S/yARaQXUBb70sCy+sWvXLs4++2xycnJQVV56\n6SUqVfLldBETS4cOubv7X3+FrCw3mmblSne3vn07nHuuG1GzaJEbHVO9Opx+OlxyCXz9tWvOeeop\nGDHCjaQJZqydP9+9t1Yt99ylixtl06oVBAYz5CHiqvuDBsFHH8HLL8OHH7rx4rbk7RGRIkRZPIDe\nwA/AamBUYN8DQJ+QY+4DHo32nFYjqNjst0pwX3zh7riTko7crQcfycmq/furLlqU9z07dqju3593\n3/ffq15+uXtfly6qGza499WqpdqyZeTaQjRyc0v+3hiZOFG1WTNVEfc8cWLxz0GMagSo6gxgRr59\n9+bbvs/LMhhjysmnn7pRN40auc7Phg3h2GPdUMmmTd2Im3C106OPLrivZUt4/XXo1cvNpm3f3tUy\n6taFTz5x5y2pckp+GE0Gg/zH9O7tRpKuX++mD4CrWIkcmXcWnAQLZde3KRo6q60CyMjI0PyL169c\nuZLf//73MSqRKQ77rWLowAF3oa5UyY1PD9eUArBhg+tQrV3bDZOsVs2Nud+zxzX1fPklfPEFrFrl\nmmZ694acHLj0UtdZ+9FHkYdslkRmJgwY4IZzzp0LzZuX3bnLQLgLPhQcnRi8mNev77bzX+CLq1mz\n4uWgE5HFqpoR9sVIVYV4fVjTUMVmv1WM5OaqXnuta2oRUe3dO29zzLx57vUTTyzYpBPu0bKlaq9e\nrqkmuC8jQ3XzZm/Kf+CA6q5d3py7EEU1yUycqFq9et5/GpHo/glL+xAp3nchVk1Dxpg48eKLrqP0\n7rtdJ+kf/+ju4O+/3zXjTJvmOmC7dnXDLdPTYedOlzphzx5XK6he3TXNnHKKm4AFrpbx73/Dd9+5\njt58Q5XLTOXKRzqMPRa8w//pp4JNMpdd5v7ZQu/q8yuvRpYyHXEaKULE6yMeawRnnXWWzpo1K8++\nsWPH6vDhwwt9X40aNVRVdcOGDTpgwICwx3Tt2lUXLlxY6HnGjh2ru3fvPrzdq1cv/e2336IpermL\n9W/lK4cOqW7apPqPf6hWqqR63nmqOTnutaeeOnJrWbOm6sMPq+7ZE9vyxlDwzr887+hL86hevfgd\nxhRSI4j5hb24j3gMBC+99JIOHTo0z75TTz1VP//880LfFwwEhYkmEDRr1kyzs7OLLmgciPVvlbB2\n7FCdMEH15ptVzz1XNTVVtXLlI1eOVq3cGP5Qzz6rOmJE6UbgxLnCmnYq2sU/WEYvRg3F/MJe3Ec8\nBoItW7ZogwYNdH+gzXXt2rXatGlTzc3N1Z07d2r37t21ffv22rp1a502bdrh9wUDwdq1azU9PV1V\nVffs2aMDBw7UVq1a6YUXXqidOnU6HAiGDx+uHTt21LS0NL333ntVVfWZZ57RypUra+vWrfWss85S\n1byB4cknn9T09HRNT0/XsWPHHv68Vq1a6TXXXKNpaWnao0cP3RPmbnD69OnaqVMnbdeunZ599tn6\nS+CCsXPnTh06dKi2bt1aTz75ZJ0yZYqqqs6cOVPbt2+vbdq00e7du4f9t4r1b5VQcnJc2/5VV6nW\nqOH+d65RQ7V9e9VBg1RHjnQX+3ffVd26NdalLRPRDKMs7AJfubJq/frlf/Ev7LNCL/DXXXfk+9Wv\n7x6lGTIayl+BYMQI1a5dy/YxYkSR/8jnnXfe4Yv8n//8Z7399ttV1c303b59u6qqZmdna/PmzTU3\nMI45XCB48skn9corr1RV1WXLlmlycvLhQLBlyxZVVc3JydGuXbvqsmXLVLVgjSC4vWjRIm3durXu\n2rVLd+7cqWlpabpkyRJdu3atJicn69dff62qqhdffLG++eabBb7T1q1bD5f15Zdf1ttuu01VVe+8\n804dEfJvsnXrVt20aZM2adJE16xZk6es+VkgKKWtW1X/9jfVgQNV69U7cvG/+mrVL7+skGPki1LY\nhT24HbxoxvruPv9nB5twQgNYWV/go1VYILDO4jIyePBgJk+eTN++fZk8eTKvvvoq4ALtPffcw9y5\nc0lKSmLDhg38+uuvNGrUKOx55s6dy8033wxAmzZtaNOmzeHX3nnnHcaPH09OTg4bN24kMzMzz+v5\nzZ8/n379+h3OgNq/f3/mzZtHnz59SE1NpV27dkDeNNahsrKyGDhwIBs3buTAgQOkpqYCLi315MmT\nDx9Xt25dPvjgA84888zDx1hSuhLYt88NvWzXLm9PYFYWzJkD77zjZsUePOjG6vfp42bXnnde+LH4\nFVikDtvgc1BwO7TTNv8xpRX8/MKGelav7nKWQeS5A+WRz6ykEi8QBFbwKm99+/bl1ltvZcmSJezZ\ns4eOHTsCLolbdnY2ixcvpnLlyqSkpJQo5fPatWt54oknWLhwIXXr1mXo0KGlSh1dNWSRi+TkZPbu\n3VvgmJtuuonbbruNPn368Nlnn3HfffeV+PN86Z//dIucXH993gv1vn0uzUL+SVG33OLSEYMbJN66\nNSxf7tIzgJuUNWIEDBzokqmV08SoslCcyVVFXfzLQ/DzmzU7UtbQ7xC8z9m6tWJd8COJl+yjFV7N\nmjXp1q0bV111FYMHDz68f/v27TRs2JDKlSszZ84cfgq3GEKIM888k7///e8AfPvttyxfvhxwKaxr\n1KhB7dq1+fXXX5k5c+bh9xx99NHs3LmzwLnOOOMMpk2bxp49e9i9ezfvvfceZ5xxRtTfafv27TRu\n3BiA119//fD+Hj16MG7cuMPbv/32G507d2bu3LmsXbsWwFYx++wz6N/frUp14oku1XBmpkt73Lix\nG8IZWF4UcLNlX3rJzaJ99lk3RHPtWpc185lnXE6edevg8cchIyNug0AwhU9SknueNCn88rCXXea+\nwjHHuIeI2xf83yNWF39wF/8333RlWLcu7wV+3Tq3Rs3mze6Rm5v3mAorUptRvD7isbM46L333lNA\nV65ceXhfdna2du7cWVu3bq1Dhw7VVq1a6dpA1sOiOov79euXp7P4iiuu0BYtWmj37t21X79+OmHC\nBFVVffbZZ7Vly5bF6iwOfp6q6uOPP65jxowp8H2mTZumqamp2qFDB73jjju0a9euquo6iy+//HJN\nT0/XNm3a6NSpU1VVdcaMGdquXTtt06aNnnPOOWH/jeLlt/LUd9+p1q2r+vvfq370keqZZx5pNK5U\nSfV//9dN3KpXzx27Y4drLG7ZssIN4czf9l2lSuFt5rFsty+sfOXZVh8r+Kqz2MS1hP+tNm1SPeEE\nl+440HGuubmq//qX6jPPqG7c6PatWuXSKKekqF5yibsi/fvfsSt3McR62GXwM6MdiRNuyGh5d9TG\nAwsEJm4k9G+1fr1qu3aqRx3lRvAUZcGCI/kJbr3V+/KVQrxc/EMv3vEwEqciKSwQJF5nsTGF2bUL\natYs/vtU4dtvXaK1TZvc48QT3YidWrVcIrZ+/Vw6hvfeg86diz7nKae4dXUnTnQLmMdIpE7QSNkv\ng89eC9dhG2rIkARom48XkSJEvD4i1QhyE3D8dKLJzc2NXY1g/36XVC05WfW116J/39dfq955p2vu\nCXerWrWqm8lbpYrLw79ihXffIUrFuVMOlzStvO7ui9OkY0qPRG8aWrNmjWZnZ1swiGO5ubmanZ19\neMJZudq4UfUPf3D/ubds6Z5feqnw9yxY4LJrguvgPfdc1fHjVRcvVs3KUt23T3X+fNVbbnHpHM4/\n37vMm1GItukm3ASssnwUNnM3niZX+VFhgSAh1iM4ePAgWVlZpRpXb7x31FFH0aRJEyqXUxZJwI3j\n79nTtXVMmOAmYQ0Y4Fb/ePZZuOmmvMcvXw6jR8MHH7i2kTvvhGuvPdJOEoeCwzNDc9+Xp0hNONHM\nHTDlJ+HXIzAmrA0b3ALpxx3nmniC9u1T7dvX3aaefLLqo4+6zt1LL3W3pnXquGycO3bEruxRCK0F\nxEPnrYlvWGex8Z2dO11H7rZtMG+eS9sQVLWqW6Hr5ZfdzKGRI93+o45yNYC77nJ59+NQpNm35aGo\nzltTcVkgMIknJwcGDYJvvnFNPKFBIKhyZZf64frrYfVqNxO4Z0836zcGCktfEFzHtjxSLwQXXAkd\nNRQujYJJMJGqCvH6sKYhn9u0yY3Xj+SHH1S7ddOoOoTLSUmWOyyrpptos3KWZKETU7FQSNOQ5Roy\nFcfq1dC+vVsg/aGH3DKJQfv3wyOPwMknw5Ilrtln2LCYFTWYcyc0h46qex427EgOnpQUt/RhWXb0\nhubKCebEUXX7mjVzZapf3z1E3L7x4+1u388SYtSQSUC7dsGhQ0fWwP3pJzjzTNi9262r+49/QFqa\nWyd33jyYOxf27oWLL3ZJ2o47LmZFj3YUT1m38QdTIdsF3YRT2KghqxGY+PPpp66tvmFDN1v3zTeh\ne3fYsQM+/himTnUpnnftckM91693WTtnz3Y5+8sxCIRm2wxm0oz2Dr8sgkBoxkwLAqakrLPYxJc3\n34Srr4aWLaFHD5g8GaZNc2kcPvnENQ2BGxH0ww8ur3/DhjEpav47/9DFUbxko3dMWbMagYkPBw/C\nmDFw+eVwxhkwfz6MHetW5/r0U/jqK5ebJ1TVquUWBMLl2R81yptJXPnb76+77kjbfqRc+caUhtUI\nTOzNm+euditWuEDw8stQpYp7LTkZunWLSbEijdkPLqxSnKadaPoDrI3fxIrVCEz5y8mBZcvcVe/i\ni10n8K5dLhPn668fCQIeCXd3n/+1olbMKk4QCN7FT5zoLvahrI3fxAOrEZjyNWMGDB0K2dluu359\nuPtud+tdo4bnH5+/XT84nDMo9LXSduZGusO3/Dsm3tjwUeOdfftcO76IGwp6//3w4IPQti386U9w\n6qnQvHm5rL8b2swTTnKyK2JphM7KtYu8iTeFDR/1tEYgIj2BZ4Bk4BVVfTTMMf8L3AcosExVL/Gy\nTKYcrF3rOn4nTnSjfVq2dKt8L17sagN//StUq+Z5MYqTl6e0QaBZM9d5a0xF5FkgEJFkYBzQA8gC\nForIdFXNDDmmBXA30EVVfxOR2IwDNMWzdi3MnOku6qGN3lu2uLv+F190t9jXX++uvt9/Dxs3unaS\na67xtAYQ6eJfVmP2VQsGlerV3d2/MRVWpNwTpX0ApwEfhmzfDdyd75i/ANcU57yWayjGvvlGtVEj\nl6AmNVV1xgzVnBzVF15QrVfPrQB27bVu8ZZy4uV6ukWtlWtpmE1FQYxyDTUGfg7ZzgrsC9USaCki\n/xaRrwJNSQWIyDARWSQii7KDnYym/C1e7NI7JCXBG2+49v/evd0wm+uuc3l+li51d/4eZ/GMdnRP\nUZKTI78Wacz+kCFuOzfXxvKbxBDr4aOVgBbAWcBg4GURqZP/IFUdr6oZqprRoEGDci6iIScH3nrL\npXmoVcuN+7/sMnfRf/BB10v697/DnDnQunWZfnS4FA6lvfiDa86ZONGNVs0/pDP4ml3kjV942Vm8\nAWgast0ksC9UFvAfVT0IrBWRH3CBYaGH5TLR2r7dXSnHjnVXxTZt4F//giZN3OtVq7pcP6NHl+nH\nRmrnD03hUJKLf2GpGWxIp/EzLwPBQqCFiKTiAsAgIP+IoGm4msAEETkG11S0xsMymaL8/LNr9pk1\nC7780g2n+cMfXDDo08fdmnso/zj/0nbyRpOXZ8gQu/Abf/MsEKhqjojcCHyIGz76mqquEJEHcJ0W\n0wOv/Y+IZAKHgD+pajml7jIF/PwznHYabNgAHTu6JRv79oVOnTz/6KLG+ReHJWUzpnhsQplxtm2D\n0093weDzz8Mv7+iRaPP3F8Yu/sYUztYjMIXbvx8uvNCldX7vvTIPApFy+5R2ha7QPD2WkdOYkrNc\nQ362f79L9Pb0064/YNIkNzKoDIXL7XPZZe7iH+0KXcHjLIWDMd6wQOAnmzbBf/7jZvquXAnTp7sF\nbY8/Hl591S37WMbC5ewvzmxfa+oxxnsWCPzixx9dkrfffnPbDRq4yWHXXgvnnFP4zKoSKG3nr+Xm\nN6b8WCDwg23b4IILXCP9p5+6PoC6dT37uNJ2/lotwJjyZYEgUWzeDJmZsGqVuw1v3x569oRKlWDg\nQFizxq35e+aZnhelpEs4Wi3AmNiwQJAIvvgCzj7b5f8PVasWtGoFCxbAK6+USRAINvmsXw/16rl9\nW7fm/buwtv/8HcQ27NOY2LPhoxXdli3ujv93v4MPP4TVq2HvXvf3RRe57TvvhKuvLvFHhEvwpuo+\nesuWgn9HEhzmaQuxGxNfbEJZRZab69r+P/nE1Qo6dizzjyiLyV5gzT7GxJpNKEtUTzzh1gB+6qky\nDwKlnewVFLzztyBgTPyyPoKKZvNmNwlsyhT46CO4+GK3ElgZKM7SjtGw5RuNqRgsEFQU27a5ZSCf\nf96tD5Ca6haAHzWqTJZ+LOusn7Z8ozEVhwWCeJebC6+9Bvfc42oDV1/tVgNr377MAkBxJ36FS/kQ\nOmrI0j8YU7FYIIh399wDjz0GXbq4NQI6dCizU5ekI9iGeRqTeCwQxLO333ZBYNgwePHFMqkBhCrO\nxC8b9WNM4rJRQ/Fq2TK48kpXE3juuTINAsERQUU1B4WmebYgYEzishpBPNq0ya0PUK+eGx1UpUqp\nT1ncEUHWBGSMf1ggiDeff+7SQW/d6v5u1KjEp4p08S8sCFgTkDH+Y01D8SInxw0P7d4datZ0M4VL\nsVZwsCM42PwTbe5/CwLG+I/VCGJt2za3KMzzz7vZV1dc4f6uWbNUpy1uBlCb/GWMf1mNIFZyc10j\nfOPGcMcdbvD9++/D3/5W6iAALjtotGzylzH+ZoEgFrKzoVcvGD3aPS9Z4voD+vQp9amDI4KKagqy\nEUHGmCBrGipvCxdCv35ulvBLL7mlIks5NDTaEUGW+98YE44FgvK0fj307n2kM7gUs4SLOyLILv7G\nmEgsEJSXvXuhf3/Yvx/mz4eTTirW2/OvDLZzJxw44F6LphnIOoKNMZFYICgPqjB8OCxe7DqESxAE\nQnMCbdlSvI8//vjiHW+M8RcLBF77+Wd4+ml44w0YM6ZEHcIlXQwebESQMaZoFgi8Mns2PPIIzJnj\nagSXXgr33luiUxVnKChYp7AxpniKHD4qIjeJSN3yKEzC+OUXlyto1SpXC1i92q3SnlSy0brRNO2E\nDge1BeGNMcURzZXpWGChiLwjIj1FyjgXciIaPdp1Cn/yiQsEJ5xQotOEZgnN/69eubJbGCa4JrBd\n/I0xJVVkIFDV0UAL4FVgKPCjiDwiIs2Lem8gcHwvIqtEZGSY14eKSLaILA08rinBd4gvS5e6FcVu\nvBFatCjxacLlCgq9658wwU1FyM21i78xpnSi6iNQVRWRX4BfgBygLjBFRD5W1TvDvUdEkoFxQA8g\nC1ermK6qmfkOfVtVbyzxN4gnqnDrrW585//9X4lOUdjSkcF2fxsKaowpS0UGAhEZAVwObAZeAf6k\nqgdFJAn4EQgbCIBOwCpVXRM4z2SgL5A/ECSO99+Hzz5zSePqFr9bJZqlI4vbcWyMMUWJpo+gHtBf\nVc9V1XdV9SCAquYC5xfyvsbAzyHbWYF9+Q0QkeUiMkVEmoY7kYgME5FFIrIoOzs7iiLHwLffwg03\nQFoa/PGPxXprsC/g0kuLHiZqcwKMMWUtmkAwE9ga3BCRWiJyKoCqrizl538ApKhqG+Bj4PVwB6nq\neFXNUNWMBg0alPIjPfDpp25JSVV46y2oFP2o3Px9AYWxOQHGGC9EEwheAHaFbO8K7CvKBiD0Dr9J\nYN9hqrpFVfcHNl8BOkZx3vgycSL07AlNm8JXX0GbNsV6e7STxSxLqDHGK9EEAlE9ks0m0CQUzS3v\nQqCFiKSKSBVgEDA9z4lFjgvZ7AOUtoZRvubOdQvJnH66yx9UjHabaBeQr17dxRobGWSM8Uo0gWCN\niNwsIpUDjxHAmqLepKo5wI3Ah7gL/DuqukJEHhCRYJ6Fm0VkhYgsA27GDU+tGLKzYfBgN0fg/feh\nTp2o3xptc5DVAowx5UG0iNSVItIQeBboDigwG7hFVTd5X7yCMjIydNGiRbH46CNyc+H8810aia++\ngvbti/X2omoCtoC8MaasichiVc0I91o0E8o2qeogVW2oqseq6iWxCgJx48knYeZMGDu2WEEgmuYg\nqwUYY8pbNPMIjgKuBtKBo9cvy0oAABCxSURBVIL7VfUqD8sVn1RdJtG77oIBA+C664p8S7Srh4FN\nFjPGxEY0fQRvAo2Ac4HPcaN/dnpZqLh08CBcfz3cdptLKPf660UuMRkuTUQkNjTUGBMr0QSCE1X1\n/4Ddqvo6cB5wqrfFijM5OW4dgRdfhDvvhClToEaNIt9mQ0ONMRVBNMNADwaet4lIa1y+oYbeFSkO\nTZ0Ks2a5ZqERI6J+WzTpIKw5yBgTa9HUCMYH1iMYjZsHkAk85mmp4omq6xxu0cJlFI1CsFO4qLWE\nrTnIGBMPCq0RBBLL7VDV34C5QMkS61dk8+fDwoXw179CcnKRhxeVOM5WDzPGxJtCawSBWcSRsov6\nwxNPuBVgrrgiqsML6xewBWSMMfEomj6CT0TkDuBtYHdwp6pujfyWBPH99/DBB27FserVo3pLpH4B\nEesLMMbEp2gCwcDA8w0h+xQ/NBONHQtVqrj00lE6/vjwE8YsfbQxJl5FM7M4Ncwj8YNAdrabK3DZ\nZXDssUUeXtj6wtYpbIyJZ9HMLL483H5VfaPsixNHXngB9u1zE8iKkL+DOLi+sHUKG2Mqgmiahk4J\n+fso4GxgCZC4gWDfPhg3Dnr3ht//vsjDw3UQ2/rCxpiKoshAoKo3hW6LSB1gsmcligcTJ8KmTXD7\n7VEdHqmD2NYXNsZUBNFMKMtvN5Ba1gWJG6rw1FPQrh1061booUVNHLMOYmNMRRBNH8EHuFFC4AJH\nGvCOl4WKqVmzYOVKeOONQpPKFTVxzDqIjTEVRTR9BE+E/J0D/KSqWR6VJ/aefBJ+9zsYOLDQw4qa\nOGYdxMaYiiKaQLAe2Kiq+wBEpJqIpKjqOk9LFgsrVrhVxx591M0fKIRNHDPGJIpo+gjeBXJDtg8F\n9iWet9+GpCS48soiD43U/m/9AsaYiiaaQFBJVQ8ENwJ/F367XFFNmQJdu0LDyFm2beKYMSbRRBMI\nskWkT3BDRPoCm70rUoxkZrpO4osuinhIuBXHgsHAFpcxxlRU0fQRDAcmicjzge0sIOxs4wptyhR3\nVe/XL+IhNnHMGJOIoplQthroLCI1A9u7PC9VLEyZAl26wHHHRTzEJo4ZYxJRkU1DIvKIiNRR1V2q\nuktE6orIQ+VRuHLzww/wzTeFNguBdRAbYxJTNH0EvVR1W3AjsFpZb++KFANTp7rn/v3DvmwdxMaY\nRBZNIEgWkarBDRGpBlQt5PiKZ8oU6NwZmjYt8JJ1EBtjEl00ncWTgNkiMgEQYCjwupeFKldr1sCS\nJW5JyjCsg9gYk+ii6Sx+TESWAefgcg59CDTzumDl5t3A3LgBA8K+bB3ExphEF2320V9xQeBioDuw\n0rMSlbfJk12zUEpK2Jetg9gYk+giBgIRaSkiY0TkO+A5XM4hUdVuqvp8pPdVKN99B0uXwqBBEQ95\n+OGC69ZbB7ExJpEUViP4Dnf3f76qnq6qz+HyDEVNRHqKyPciskpERhZy3AARURHJKM75S+3tt13P\n78UXF3gpOFLossugWjWoX98dah3ExphEU1gfQX9gEDBHRGbhViWLnKA/HxFJBsYBPXCzkReKyHRV\nzcx33NHACOA/xSx76ai6ZqEzz3Rpp0PkX2tgyxZXC3jzTQsAxpjEE7FGoKrTVHUQ0AqYA9wCNBSR\nF0Tkf6I4dydglaquCSSqmwz0DXPcg8BjwL5il740li93TUNhmoXCjRTas8ftN8aYRFNkZ7Gq7lbV\nv6vqBUAT4GvgrijO3Rj4OWQ7K7DvMBHpADRV1X8VdiIRGSYii0RkUXZ2dhQfHYXJkyE5OexoIRsp\nZIzxk2KtWayqv6nqeFU9u7QfLCJJwFNAkSvEBz4zQ1UzGjRoUNqPds1Cb78N55wDYc5nI4WMMX5S\nksXro7UBCJ2q2ySwL+hooDXwmYisAzoD08ulw3jhQli7NuJylDZSyBjjJ14GgoVACxFJFZEquI7n\n6cEXVXW7qh6jqimqmgJ8BfRR1UUelsn597/d83nnhX15yBA3MqhZMxspZIxJfNGkmCgRVc0RkRtx\nM5GTgddUdYWIPAAsUtXphZ/BQ6tXQ61aYZuFgoYMsQu/McYfvKwRoKozVLWlqjZX1YcD++4NFwRU\n9axyqQ2Ayy/UvHmBVKLBuQNJSe550qRyKY0xxsSUZzWCuLZ6NbRpk2dX/rkDP/3ktsFqBsaYxOZp\njSAuHTrkOoqbN8+z2+YOGGP8yn+BICsLDh6EE07Is9vmDhhj/Mp/gWDNGvecr0ZgcweMMX7lv0Cw\nerV7zhcIbO6AMcav/BkIKlWCJk3y7La5A8YYv/LfqKE1a9zY0EoFv7rNHTDG+JE/awT5moWMMcbP\n/BkI8o0YMsYYP/NXINi6FbZtsxqBMcaE8FcgyDd01FJKGGOM3zqLg0NHTzjBUkoYY0yAv2oEIYHA\nUkoYY4zjr0CwZg0ceyzUrGkpJYwxJsBfgSBk6KillDDGGMd/gSAwdNRSShhjjOOfQLB/v8s8GqgR\nWEoJY4xx/DNqaN06UM0zh8BSShhjjJ9qBCEjhowxxhzhv0Bgs4qNMSYP/wSC9HS4/no3fNQYY8xh\n/ukj6N7dPYwxxuThnxqBMcaYsCwQGGOMz1kgMMYYn/NdILDU08YYk5d/OovBUk8bY0wYvqoRWOpp\nY4wpyFeBwFJPG2NMQb4KBJZ62hhjCvI0EIhITxH5XkRWicjIMK8PF5FvRGSpiMwXkTQvy2Opp40x\npiDPAoGIJAPjgF5AGjA4zIX+76p6sqq2A/4CPOVVecBSTxtjTDhejhrqBKxS1TUAIjIZ6AtkBg9Q\n1R0hx9cA1MPyAJZ62hhj8vMyEDQGfg7ZzgJOzX+QiNwA3AZUAcImAxKRYcAwgOOtQd8YY8pUzDuL\nVXWcqjYH7gJGRzhmvKpmqGpGgwYNyreAxhiT4LwMBBuApiHbTQL7IpkMXOhheYwxxoThZSBYCLQQ\nkVQRqQIMAqaHHiAiLUI2zwN+9LA8xhhjwvCsj0BVc0TkRuBDIBl4TVVXiMgDwCJVnQ7cKCLnAAeB\n34ArvCqPMcaY8DzNNaSqM4AZ+fbdG/L3CC8/3xhjTNFi3llsjDEmtiwQGGOMz1kgMMYYn7NAYIwx\nPmeBwBhjfM4CgTHG+JwFAmOM8TkLBMYY43MWCIwxxucsEBhjjM9ZIDDGGJ+zQGCMMT5ngcAYY3zO\nAoExxvicBQJjjPE5CwTGGONzFgiMMcbnLBAYY4zPWSAwxhifs0BgjDE+Z4HAGGN8zgKBMcb4nAUC\nY4zxOQsExhjjcxYIjDHG5ywQGGOMz1kgMMYYn/NFIJg0CVJSICnJPU+aFOsSGWNM/KgU6wJ4bdIk\nGDYM9uxx2z/95LYBhgyJXbmMMSZeJHyNYNSoI0EgaM8et98YY4zHgUBEeorI9yKySkRGhnn9NhHJ\nFJHlIjJbRJqVdRnWry/efmOM8RvPAoGIJAPjgF5AGjBYRNLyHfY1kKGqbYApwF/KuhzHH1+8/cYY\n4zde1gg6AatUdY2qHgAmA31DD1DVOaoabLj5CmhS1oV4+GGoXj3vvurV3X5jjDHeBoLGwM8h21mB\nfZFcDcwM94KIDBORRSKyKDs7u1iFGDIExo+HZs1AxD2PH28dxcYYExQXo4ZE5FIgA+ga7nVVHQ+M\nB8jIyNDinn/IELvwG2NMJF4Ggg1A05DtJoF9eYjIOcAooKuq7vewPMYYY8LwsmloIdBCRFJFpAow\nCJgeeoCItAdeAvqo6iYPy2KMMSYCzwKBquYANwIfAiuBd1R1hYg8ICJ9Aoc9DtQE3hWRpSIyPcLp\njDHGeMTTPgJVnQHMyLfv3pC/z/Hy840xxhQt4WcWG2OMKZyoFnsQTkyJSDbwUwnffgywuQyLU1H4\n8Xv78TuDP7+3H78zFP97N1PVBuFeqHCBoDREZJGqZsS6HOXNj9/bj98Z/Pm9/fidoWy/tzUNGWOM\nz1kgMMYYn/NbIBgf6wLEiB+/tx+/M/jze/vxO0MZfm9f9REYY4wpyG81AmOMMflYIDDGGJ/zTSAo\narW0RCAiTUVkTmDVtxUiMiKwv56IfCwiPwae68a6rGVNRJJF5GsR+WdgO1VE/hP4vd8O5LtKKCJS\nR0SmiMh3IrJSRE7zyW99a+C/729F5C0ROSrRfm8ReU1ENonItyH7wv624jwb+O7LRaRDcT/PF4Eg\nytXSEkEOcLuqpgGdgRsC33MkMFtVWwCzA9uJZgQup1XQY8BYVT0R+A233kWieQaYpaqtgLa475/Q\nv7WINAZuxq1s2BpIxiW0TLTf+29Az3z7Iv22vYAWgccw4IXifpgvAgFRrJaWCFR1o6ouCfy9E3dh\naIz7rq8HDnsduDA2JfSGiDQBzgNeCWwL0B23/Ckk5neuDZwJvAqgqgdUdRsJ/lsHVAKqiUgloDqw\nkQT7vVV1LrA13+5Iv21f4A11vgLqiMhxxfk8vwSC4q6WVuGJSArQHvgPcKyqbgy89AtwbIyK5ZWn\ngTuB3MB2fWBbIAMuJObvnQpkAxMCTWKviEgNEvy3VtUNwBPAelwA2A4sJvF/b4j825b6+uaXQOAr\nIlITmArcoqo7Ql9TN144YcYMi8j5wCZVXRzrspSzSkAH4AVVbQ/sJl8zUKL91gCBdvG+uED4O6AG\nBZtQEl5Z/7Z+CQRRrZaWCESkMi4ITFLVfwR2/xqsKgaeE2kRoC5AHxFZh2vy645rO68TaDqAxPy9\ns4AsVf1PYHsKLjAk8m8NcA6wVlWzVfUg8A/cfwOJ/ntD5N+21Nc3vwSCIldLSwSBtvFXgZWq+lTI\nS9OBKwJ/XwG8X95l84qq3q2qTVQ1Bfe7fqqqQ4A5wEWBwxLqOwOo6i/AzyJyUmDX2UAmCfxbB6wH\nOotI9cB/78HvndC/d0Ck33Y6cHlg9FBnYHtIE1J0VNUXD6A38AOwGhgV6/J49B1Px1UXlwNLA4/e\nuDbz2cCPwCdAvViX1aPvfxbwz8DfJwALgFXAu0DVWJfPg+/bDlgU+L2nAXX98FsD9wPfAd8CbwJV\nE+33Bt7C9YEcxNX+ro702wKCGxW5GvgGN6KqWJ9nKSaMMcbn/NI0ZIwxJgILBMYY43MWCIwxxucs\nEBhjjM9ZIDDGGJ+zQGBMgIgcEpGlIY8yS9gmIimhmSSNiSeVij7EGN/Yq6rtYl0IY8qb1QiMKYKI\nrBORv4jINyKyQERODOxPEZFPAzngZ4vI8YH9x4rIeyKyLPD4Q+BUySLyciCX/kciUi1w/M2BNSSW\ni8jkGH1N42MWCIw5olq+pqGBIa9tV9WTgedx2U4BngNeV9U2wCTg2cD+Z4HPVbUtLv/PisD+FsA4\nVU0HtgEDAvtHAu0D5xnu1ZczJhKbWWxMgIjsUtWaYfavA7qr6ppAUr9fVLW+iGwGjlPVg4H9G1X1\nGBHJBpqo6v6Qc6QAH6tbVAQRuQuorKoPicgsYBcuTcQ0Vd3l8Vc1Jg+rERgTHY3wd3HsD/n7EEf6\n6M7D5YrpACwMyaJpTLmwQGBMdAaGPH8Z+PsLXMZTgCHAvMDfs4Hr4PBayrUjnVREkoCmqjoHuAuo\nDRSolRjjJbvzMOaIaiKyNGR7lqoGh5DWFZHluLv6wYF9N+FWCPsTbrWwKwP7RwDjReRq3J3/dbhM\nkuEkAxMDwUKAZ9UtOWlMubE+AmOKEOgjyFDVzbEuizFesKYhY4zxOasRGGOMz1mNwBhjfM4CgTHG\n+JwFAmOM8TkLBMYY43MWCIwxxuf+H+MtZVp3jB9yAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SomXA-mLoMtr",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train (again) and evaluate the model\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWstzZusoMtt",
        "colab_type": "text"
      },
      "source": [
        "### 3.1. Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOEX3Q9OoMtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <Compile your model again (using the same hyper-parameters)>\n",
        "# ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlyBeDGExgQo",
        "colab_type": "code",
        "outputId": "8e0f935d-8257-497e-ecb7-f5442e818d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_generator = datagen.flow(x_train, y_train_vec, batch_size=32)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(train_generator, \n",
        "                              epochs=100)\n",
        "\n",
        "print(\"\\n\\n*********************************************************************\")\n",
        "print(\"loss: {0}, acc: {1}\".format(history.history['loss'][-1], history.history['acc'][-1]))\n",
        "print(\"*********************************************************************\\n\\n\")   "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.9173 - acc: 0.6810\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.9158 - acc: 0.6806\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.9111 - acc: 0.6804\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.9043 - acc: 0.6817\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8989 - acc: 0.6845\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.9039 - acc: 0.6841\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8926 - acc: 0.6854\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8985 - acc: 0.6868\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8866 - acc: 0.6889\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8891 - acc: 0.6888\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8892 - acc: 0.6878\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8862 - acc: 0.6901\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8795 - acc: 0.6909\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8775 - acc: 0.6936\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8745 - acc: 0.6977\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8708 - acc: 0.6971\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8773 - acc: 0.6941\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8648 - acc: 0.6982\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8641 - acc: 0.6990\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8598 - acc: 0.6994\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8653 - acc: 0.6982\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8611 - acc: 0.6998\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8580 - acc: 0.7001\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8528 - acc: 0.7021\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8471 - acc: 0.7029\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8463 - acc: 0.7046\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8479 - acc: 0.7024\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8403 - acc: 0.7070\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8406 - acc: 0.7056\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8470 - acc: 0.7051\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8391 - acc: 0.7068\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8338 - acc: 0.7093\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8298 - acc: 0.7076\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8331 - acc: 0.7100\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8286 - acc: 0.7107\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8280 - acc: 0.7123\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8257 - acc: 0.7120\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8242 - acc: 0.7102\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8220 - acc: 0.7120\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8170 - acc: 0.7151\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8136 - acc: 0.7165\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8155 - acc: 0.7135\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8133 - acc: 0.7153\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8142 - acc: 0.7174\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8076 - acc: 0.7190\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8003 - acc: 0.7212\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8102 - acc: 0.7178\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.8076 - acc: 0.7195\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8006 - acc: 0.7203\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7959 - acc: 0.7223\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7989 - acc: 0.7208\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7965 - acc: 0.7228\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7899 - acc: 0.7257\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7902 - acc: 0.7243\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7957 - acc: 0.7234\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7962 - acc: 0.7235\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7862 - acc: 0.7241\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7886 - acc: 0.7248\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7834 - acc: 0.7276\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7791 - acc: 0.7302\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7792 - acc: 0.7267\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7787 - acc: 0.7302\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7801 - acc: 0.7276\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7775 - acc: 0.7295\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7729 - acc: 0.7312\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7707 - acc: 0.7328\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7733 - acc: 0.7302\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7709 - acc: 0.7320\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7711 - acc: 0.7326\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7706 - acc: 0.7323\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7616 - acc: 0.7338\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7684 - acc: 0.7303\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.7672 - acc: 0.7355\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7616 - acc: 0.7333\n",
            "Epoch 75/100\n",
            " 173/1563 [==>...........................] - ETA: 44s - loss: 0.7674 - acc: 0.7321Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5cFxnbtoMt4",
        "colab_type": "text"
      },
      "source": [
        "### 3.2. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEcTMRHvoMt5",
        "colab_type": "code",
        "outputId": "24aee472-68af-4ffb-9290-dde8da560e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 228us/step\n",
            "loss = 0.588442866897583\n",
            "accuracy = 0.7983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Q1ldcKoMt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}